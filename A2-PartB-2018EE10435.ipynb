{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python387jvsc74a57bd02db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37",
   "display_name": "Python 3.8.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "source": [
    "## Plot Function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(values, policy, goalState, flag):\n",
    "    cmap = plt.cm.gray\n",
    "    norm = plt.Normalize(np.min(values), np.max(values))\n",
    "    rgba = cmap(norm(values))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(rgba, interpolation = 'nearest')\n",
    "    ax.set_ylim(ax.get_ylim()[::-1])\n",
    "    print(np.min(values), np.max(values))\n",
    "\n",
    "    if(flag):\n",
    "        for i in range(0, 25):\n",
    "            for j in range(0, 50):\n",
    "                #print('policy for ', i, j, policy[i][j])\n",
    "                if(not isValid(j, i)): continue\n",
    "\n",
    "                num = int(policy[i][j])\n",
    "                text = ''\n",
    "                if(num == 1): text = 'L'\n",
    "                elif(num == 2): text = 'U'\n",
    "                elif(num == 3): text = 'R'\n",
    "                else: text = 'D'\n",
    "                text = ax.text(j, i, text, ha = 'center', va = 'center', color = 'red', size = 'xx-small')\n",
    "\n",
    "    rgba[12, 48] = 1.0, 0.0, 0.0, 1.0\n",
    "    plt.axis('on')\n",
    "    plt.show()"
   ]
  },
  {
   "source": [
    "## Part a - Implement Q - Learning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseAction(values, x, y):\n",
    "    num = random.randint(1, 100)\n",
    "    if(num <= 95):\n",
    "        currMax = -np.Inf\n",
    "        actionTaken = -1\n",
    "        for action in range(0, 4):\n",
    "            if(values[x][y][action] > currMax):\n",
    "                actionTaken = action\n",
    "                currMax = values[x][y][action]\n",
    "        \n",
    "        return actionTaken\n",
    "    \n",
    "    else:\n",
    "        return random.randint(0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def takeAction(x, y, action, walls, goalState):\n",
    "    # 0 is left, 1 is up, 2 is right, 3 is down\n",
    "    num = random.uniform(0.0, 1.0)\n",
    "\n",
    "    newStateX = -5\n",
    "    newStateY = -5\n",
    "\n",
    "    if(action == 0):\n",
    "        if(num <= 0.8): newStateX = x - 1\n",
    "        elif(num <= 0.8 + 0.2 / 3): newStateY = y + 1\n",
    "        elif(num <= 1.0 - 0.2 / 3): newStateX = x + 1\n",
    "        else: newStateY = y - 1\n",
    "    if(action == 1):\n",
    "        if(num <= 0.8): newStateY = y + 1\n",
    "        elif(num <= 0.8 + 0.2 / 3): newStateX = x - 1\n",
    "        elif(num <= 1.0 - 0.2 / 3): newStateX = x + 1\n",
    "        else: newStateY = y - 1\n",
    "    if(action == 2):\n",
    "        if(num <= 0.8): newStateX = x + 1\n",
    "        elif(num <= 0.8 + 0.2 / 3): newStateY = y + 1\n",
    "        elif(num <= 1.0 - 0.2 / 3): newStateX = x - 1\n",
    "        else: newStateY = y - 1\n",
    "    if(action == 3):\n",
    "        if(num <= 0.8): newStateY = y - 1\n",
    "        elif(num <= 0.8 + 0.2 / 3): newStateY = y + 1\n",
    "        elif(num <= 1.0 - 0.2 / 3): newStateX = x + 1\n",
    "        else: newStateX = x - 1\n",
    "    \n",
    "    if(newStateX == -5): newStateX = x\n",
    "    if(newStateY == -5): newStateY = y\n",
    "\n",
    "    if(walls[newStateX][newStateY] == 1):\n",
    "        return -1, x, y\n",
    "    if(newStateX == 48 and newStateY == 12):\n",
    "        return 100, 48, 12\n",
    "    return 0, newStateX, newStateY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qLearn(values, walls, goalState, alpha, gamma, epsilon, maxIter, xInit, yInit, rewards, episode):\n",
    "\n",
    "    xCurr = xInit\n",
    "    yCurr = yInit\n",
    "    aCurr = chooseAction(values, xInit, yInit)\n",
    "\n",
    "    iter = 0\n",
    "    rewardAcc = 0\n",
    "    while(iter < maxIter):\n",
    "        if(xCurr == 48 and yCurr == 12): break\n",
    "\n",
    "        reward, xNew, yNew = takeAction(xCurr, yCurr, aCurr)\n",
    "        rewardAcc += reward\n",
    "\n",
    "        quantity = -np.Inf\n",
    "        for action in range(0, 4):\n",
    "            if(values[xNew][yNew][action] > currMax):\n",
    "                quantity = values[xNew][yNew][action]\n",
    "\n",
    "        values[xCurr][yCurr][aCurr] = values[xCurr][yCurr][aCurr] + alpha * (reward + gamma * quantity - values[xCurr][yCurr][aCurr])\n",
    "\n",
    "        xCurr = xNew\n",
    "        yCurr = yNew\n",
    "        aCurr = chooseAction(values, xCurr, yCurr)\n",
    "        iter += 1\n",
    "    \n",
    "    rewards[episode] = rewardAcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np.zeros((50, 25, 4))\n",
    "rewards = np.zeros((4000))\n",
    "walls = np.zeros((50, 25))\n",
    "\n",
    "for i in range(0, 50):\n",
    "    for j in range(0, 25):\n",
    "        if(i == 0 or j == 0 or i == 49 or j == 24): walls[i][j] = 1\n",
    "        if(i == 25 or i == 26):\n",
    "            if(j <= 11 or j >= 13): walls[i][j] = 1\n",
    "\n",
    "for i in range(0, 50):\n",
    "    for j in range(0, 25):\n",
    "        for k in range(0, 4):\n",
    "            if(walls[i][j] == 1): break\n",
    "            if(i == 48 and j == 12): break\n",
    "            values[i][j][k] = random.uniform(0.0, 1.0)\n",
    "\n",
    "for episode in range(0, 4000):\n",
    "    xInit = random.randint(1, 48)\n",
    "    yInit = random.randint(1, 23)\n",
    "\n",
    "    if(xInit == 48 and yInit = 12): xInit = random.randint(1, 47)\n",
    "    if(walls[xInit][yInit] == 1): yInit = 12\n",
    "\n",
    "    values, rewards = qLearn(values, walls, (48, 12), 0.25, 0.99, 0.05, 1000, xInit, yInit, rewards, episode)"
   ]
  },
  {
   "source": [
    "## Part b - Visualization of state-value pairs and optimal policy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.zeros((50, 25))\n",
    "toPlot = np.zeros((50, 25))\n",
    "\n",
    "for i in range(0, 50):\n",
    "    for j in range(0, 25):\n",
    "        if(walls[i][j] == 1): continue\n",
    "        if(i == 48 and j == 12): continue\n",
    "        \n",
    "        currMax = -np.Inf\n",
    "        actionTaken = -1\n",
    "        for action in range(0, 4):\n",
    "            if(values[i][j][action] > currMax):\n",
    "                actionTaken = action + 1\n",
    "                currMax = values[x][y][action]\n",
    "        \n",
    "        policy[i][j] = actionTaken\n",
    "        toPlot[i][j] = currMax\n",
    "\n",
    "plot(toPlot.transpose(), policy.transpose(), (48, 12), True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-Learning for epsilon = 0.005\n",
    "values_C1 = np.zeros((50, 25, 4))\n",
    "rewards_C1 = np.zeros((4000))\n",
    "\n",
    "for i in range(0, 50):\n",
    "    for j in range(0, 25):\n",
    "        if(i == 0 or j == 0 or i == 49 or j == 24): walls[i][j] = 1\n",
    "        if(i == 25 or i == 26):\n",
    "            if(j <= 11 or j >= 13): walls[i][j] = 1\n",
    "\n",
    "for i in range(0, 50):\n",
    "    for j in range(0, 25):\n",
    "        for k in range(0, 4):\n",
    "            if(walls[i][j] == 1): break\n",
    "            if(i == 48 and j == 12): break\n",
    "            values_C1[i][j][k] = random.uniform(0.0, 1.0)\n",
    "\n",
    "for episode in range(0, 4000):\n",
    "    xInit = random.randint(1, 48)\n",
    "    yInit = random.randint(1, 23)\n",
    "    \n",
    "    if(xInit == 48 and yInit = 12): xInit = random.randint(1, 47)\n",
    "    if(walls[xInit][yInit] == 1): yInit = 12\n",
    "\n",
    "    values_C1, rewards_C1 = qLearn(values_C1, walls, (48, 12), 0.25, 0.99, 0.005, 1000, xInit, yInit, rewards_C1, episode)\n",
    "\n",
    "#Q-Learning for epsilon = 0.5 ---------------------------------------------------------------------------------------------------------------------\n",
    "values_C3 = np.zeros((50, 25, 4))\n",
    "rewards_C3 = np.zeros((4000))\n",
    "\n",
    "for i in range(0, 50):\n",
    "    for j in range(0, 25):\n",
    "        if(i == 0 or j == 0 or i == 49 or j == 24): walls[i][j] = 1\n",
    "        if(i == 25 or i == 26):\n",
    "            if(j <= 11 or j >= 13): walls[i][j] = 1\n",
    "\n",
    "for i in range(0, 50):\n",
    "    for j in range(0, 25):\n",
    "        for k in range(0, 4):\n",
    "            if(walls[i][j] == 1): break\n",
    "            if(i == 48 and j == 12): break\n",
    "            values_C3[i][j][k] = random.uniform(0.0, 1.0)\n",
    "\n",
    "for episode in range(0, 4000):\n",
    "    xInit = random.randint(1, 48)\n",
    "    yInit = random.randint(1, 23)\n",
    "    \n",
    "    if(xInit == 48 and yInit = 12): xInit = random.randint(1, 47)\n",
    "    if(walls[xInit][yInit] == 1): yInit = 12\n",
    "\n",
    "    values_C3, rewards_C3 = qLearn(values_C3, walls, (48, 12), 0.25, 0.99, 0.5, 1000, xInit, yInit, rewards_C3, episode)"
   ]
  },
  {
   "source": [
    "## Part d - Plotting reward accumulated per episode"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards)\n",
    "plt.plot(rewards_C3)"
   ]
  }
 ]
}